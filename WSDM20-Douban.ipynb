{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import shutil\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from collections import Counter\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Data partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.6 # train:val:test = ratio:(1-ratio)/2:(1-ratio)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('douban.mat')\n",
    "UM, UU, UG, UL, MD, MA, MT = (x.tocoo() for x in list(mat['relation'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_biz = defaultdict(list)\n",
    "for u, b, d in zip(UM.row, UM.col, UM.data):\n",
    "    if d >= 5:\n",
    "        user_biz[u].append(b)\n",
    "\n",
    "user_biz_train = defaultdict(list)\n",
    "user_biz_val = defaultdict(list)\n",
    "user_biz_test = defaultdict(list)\n",
    "train_set = set()\n",
    "val_set = set()\n",
    "test_set = set()\n",
    "for u in user_biz:\n",
    "    if len(user_biz[u]) >= 5:\n",
    "        val = int(len(user_biz[u]) * ratio)\n",
    "        test = int(len(user_biz[u]) * (ratio+(1-ratio)/2))\n",
    "        random.shuffle(user_biz[u])\n",
    "        user_biz_train[u] = user_biz[u][:val]\n",
    "        for b in user_biz[u][:val]:\n",
    "            train_set.add((u, b))\n",
    "        user_biz_val[u] = user_biz[u][val:test]\n",
    "        for b in user_biz[u][val:test]:\n",
    "            val_set.add((u, b))\n",
    "        user_biz_test[u] = user_biz[u][test:]\n",
    "        for b in user_biz[u][test:]:\n",
    "            test_set.add((u, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = []\n",
    "val_mask = []\n",
    "test_mask = []\n",
    "for ind, ub in enumerate(zip(UM.row, UM.col)):\n",
    "    if ub in train_set:\n",
    "        train_mask.append(ind)\n",
    "    if ub in val_set:\n",
    "        val_mask.append(ind)\n",
    "    if ub in test_set:\n",
    "        test_mask.append(ind)\n",
    "train_mask = np.array(train_mask)\n",
    "val_mask = np.array(val_mask)\n",
    "test_mask = np.array(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_biz_train = np.zeros([UM.shape[0], UM.shape[1]])\n",
    "for u, b in zip(UM.row[train_mask], UM.col[train_mask]):\n",
    "    user_biz_train[u, b] += 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tmp = np.sum(user_biz_train, -1)\n",
    "user_non_zero_ind = np.array([i for i in range(UM.shape[0]) if sum_tmp[i] > 0])\n",
    "user_non_zero_set = set(user_non_zero_ind)\n",
    "\n",
    "sum_tmp = np.sum(user_biz_train[user_non_zero_ind, :], 0)\n",
    "biz_non_zero_mask = (sum_tmp != 0).astype(float)\n",
    "biz_non_zero_set = set([i for i in range(UM.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_biz_val = np.zeros([UM.shape[0], UM.shape[1]])\n",
    "user_biz_val_dict = defaultdict(list)\n",
    "for u, b in zip(UM.row[val_mask], UM.col[val_mask]):\n",
    "    if u in user_non_zero_set and b in biz_non_zero_set:\n",
    "        user_biz_val[u, b] += 1.\n",
    "    user_biz_val_dict[u].append(b)\n",
    "\n",
    "user_biz_test = np.zeros([UM.shape[0], UM.shape[1]])\n",
    "user_biz_test_dict = defaultdict(list)\n",
    "for u, b in zip(UM.row[test_mask], UM.col[test_mask]):\n",
    "    if u in user_non_zero_set and b in biz_non_zero_set:\n",
    "        user_biz_test[u, b] += 1.\n",
    "    user_biz_test_dict[u].append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data structure needed for the convenience of evaluation\n",
    "val_set_u = list(user_non_zero_set)\n",
    "val_set_mask = []\n",
    "val_set_set = []\n",
    "for user in val_set_u:\n",
    "    val_set_mask.append(biz_non_zero_mask - user_biz_train[user, :] - user_biz_test[user, :])\n",
    "    val_set_set.append(set(user_biz_val_dict[user]))\n",
    "\n",
    "test_set_u = list(user_non_zero_set)\n",
    "test_set_mask = []\n",
    "test_set_set = []\n",
    "for user in test_set_u:\n",
    "    test_set_mask.append(biz_non_zero_mask - user_biz_train[user, :] - user_biz_val[user, :])\n",
    "    test_set_set.append(set(user_biz_test_dict[user]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the training data\n",
    "user_biz_train = user_biz_train[user_non_zero_ind, :]\n",
    "user_biz_train = user_biz_train/np.sum(user_biz_train, -1).reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### HINs & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse tensors for adjacency matrices\n",
    "\n",
    "UM_train = tf.SparseTensor(indices=np.array([UM.row[train_mask], UM.col[train_mask]]).transpose(),\n",
    "                           values=np.ones(len(train_mask)).astype(np.float32),\n",
    "                           dense_shape=UM.shape)\n",
    "UU_t = tf.SparseTensor(indices=np.array([UU.row, UU.col]).transpose(), values=UU.data.astype(np.float32), dense_shape=UU.shape)\n",
    "UG_t = tf.SparseTensor(indices=np.array([UG.row, UG.col]).transpose(), values=UG.data.astype(np.float32), dense_shape=UG.shape)\n",
    "UL_t = tf.SparseTensor(indices=np.array([UL.row, UL.col]).transpose(), values=UL.data.astype(np.float32), dense_shape=UL.shape)\n",
    "MD_t = tf.SparseTensor(indices=np.array([MD.row, MD.col]).transpose(), values=MD.data.astype(np.float32), dense_shape=MD.shape)\n",
    "MA_t = tf.SparseTensor(indices=np.array([MA.row, MA.col]).transpose(), values=MA.data.astype(np.float32), dense_shape=MA.shape)\n",
    "MT_t = tf.SparseTensor(indices=np.array([MT.row, MT.col]).transpose(), values=MT.data.astype(np.float32), dense_shape=MT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paras\n",
    "\n",
    "num_U = UU.shape[0]\n",
    "num_G = UG.shape[1]\n",
    "num_L = UL.shape[1]\n",
    "num_M = MD.shape[0]\n",
    "num_A = MA.shape[1]\n",
    "num_D = MD.shape[1]\n",
    "num_T = MT.shape[1]\n",
    "\n",
    "dim = 64\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "U_embeddings = tf.get_variable(\"U_embeddings\", [num_U, dim], initializer=tf.truncated_normal_initializer(stddev=0.01), regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "M_embeddings = tf.get_variable(\"B_embeddings\", [num_M, dim], initializer=tf.truncated_normal_initializer(stddev=0.01), regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "G_embeddings = tf.get_variable(\"G_embeddings\", [num_G, dim], initializer=tf.truncated_normal_initializer(stddev=0.01), regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "L_embeddings = tf.get_variable(\"L_embeddings\", [num_L, dim], initializer=tf.truncated_normal_initializer(stddev=0.01), regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "A_embeddings = tf.get_variable(\"A_embeddings\", [num_A, dim], initializer=tf.truncated_normal_initializer(stddev=0.01), regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "D_embeddings = tf.get_variable(\"D_embeddings\", [num_D, dim], initializer=tf.truncated_normal_initializer(stddev=0.01), regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "T_embeddings = tf.get_variable(\"T_embeddings\", [num_T, dim], initializer=tf.truncated_normal_initializer(stddev=0.01), regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user & item lookup table\n",
    "U_vec = U_embeddings + tf.sparse_tensor_dense_matmul(tf.sparse_softmax(UM_train), M_embeddings) + \\\n",
    "        tf.sparse_tensor_dense_matmul(tf.sparse_softmax(UG_t), G_embeddings) + \\\n",
    "        tf.sparse_tensor_dense_matmul(tf.sparse_softmax(UU_t), U_embeddings) + \\\n",
    "        tf.sparse_tensor_dense_matmul(tf.sparse_softmax(UL_t), L_embeddings)\n",
    "U_vec = tf.nn.tanh(U_vec)\n",
    "\n",
    "M_vec = M_embeddings + \\\n",
    "        tf.sparse_tensor_dense_matmul(tf.sparse_softmax(MA_t), A_embeddings) + \\\n",
    "        tf.sparse_tensor_dense_matmul(tf.sparse_softmax(MD_t), D_embeddings) + \\\n",
    "        tf.sparse_tensor_dense_matmul(tf.sparse_softmax(MT_t), T_embeddings)\n",
    "M_vec = tf.nn.tanh(M_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders\n",
    "ux = tf.placeholder(tf.int32, shape=(None,))\n",
    "uy = tf.placeholder(tf.float32, shape=(None, num_M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvec = tf.tile(tf.expand_dims(tf.nn.embedding_lookup(U_vec, ux), 1), [1, num_M, 1])\n",
    "bvec = tf.tile(tf.expand_dims(tf.nn.embedding_lookup(M_vec, tf.range(0, num_M)), 0), [tf.shape(uvec)[0], 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-40331a0b7bf4>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tf.concat([uvec * bvec], axis=-1)\n",
    "y_emb_logit = tf.squeeze(tf.layers.dense(x, 1, name='output_2', kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1)))\n",
    "y_inference = tf.nn.softmax(y_emb_logit, -1)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_emb_logit, labels=uy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learnable adjacency matrices\n",
    "\n",
    "UM_e = tf.Variable(np.zeros(shape=len(train_mask)), trainable=True, dtype=tf.float32)\n",
    "UU_e = tf.Variable(np.zeros(shape=len(UU.row)), trainable=True, dtype=tf.float32)\n",
    "UG_e = tf.Variable(np.zeros(shape=len(UG.row)), trainable=True, dtype=tf.float32)\n",
    "UL_e = tf.Variable(np.zeros(shape=len(UL.row)), trainable=True, dtype=tf.float32)\n",
    "MD_e = tf.Variable(np.zeros(shape=len(MD.row)), trainable=True, dtype=tf.float32)\n",
    "MA_e = tf.Variable(np.zeros(shape=len(MA.row)), trainable=True, dtype=tf.float32)\n",
    "MT_e = tf.Variable(np.zeros(shape=len(MT.row)), trainable=True, dtype=tf.float32)\n",
    "M_p = tf.Variable(np.zeros(shape=num_M), trainable=True, dtype=tf.float32)\n",
    "\n",
    "UM_t = tf.SparseTensor(indices=np.array([UM.row[train_mask], UM.col[train_mask]]).transpose(),\n",
    "                       values=tf.nn.softplus(UM_e),\n",
    "                       dense_shape=UM.shape)\n",
    "UU_t = tf.SparseTensor(indices=np.array([UU.row, UU.col]).transpose(),\n",
    "                       values=tf.nn.softplus(UU_e),\n",
    "                       dense_shape=UU.shape)\n",
    "UG_t = tf.SparseTensor(indices=np.array([UG.row, UG.col]).transpose(),\n",
    "                       values=tf.nn.softplus(UG_e),\n",
    "                       dense_shape=UG.shape)\n",
    "UL_t = tf.SparseTensor(indices=np.array([UL.row, UL.col]).transpose(),\n",
    "                       values=tf.nn.softplus(UL_e),\n",
    "                       dense_shape=UL.shape)\n",
    "MD_t = tf.SparseTensor(indices=np.array([MD.row, MD.col]).transpose(),\n",
    "                       values=tf.nn.softplus(MD_e),\n",
    "                       dense_shape=MD.shape)\n",
    "MA_t = tf.SparseTensor(indices=np.array([MA.row, MA.col]).transpose(),\n",
    "                       values=tf.nn.softplus(MA_e),\n",
    "                       dense_shape=MA.shape)\n",
    "MT_t = tf.SparseTensor(indices=np.array([MT.row, MT.col]).transpose(),\n",
    "                       values=tf.nn.softplus(MT_e),\n",
    "                       dense_shape=MT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_one_hot = tf.one_hot(ux, depth=num_U)\n",
    "# meta-paths\n",
    "\n",
    "# P1: UMUM\n",
    "M_1 = tf.transpose(tf.sparse_tensor_dense_matmul(UM_t, u_one_hot, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "M_1 = M_1 / tf.reshape(tf.reduce_sum(M_1, axis=1), [-1, 1])\n",
    "U_1 = tf.transpose(tf.sparse_tensor_dense_matmul(UM_t, M_1, adjoint_b=True)) + 1e-10\n",
    "U_1 = U_1 / tf.reshape(tf.reduce_sum(U_1, axis=1), [-1, 1])\n",
    "M_2 = tf.transpose(tf.sparse_tensor_dense_matmul(UM_t, U_1, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "M_2 = M_2 / tf.reshape(tf.reduce_sum(M_2, axis=1), [-1, 1])\n",
    "\n",
    "# P2: UMUMUM\n",
    "U_4 = tf.transpose(tf.sparse_tensor_dense_matmul(UM_t, M_2, adjoint_b=True)) + 1e-10\n",
    "U_4 = U_4 / tf.reshape(tf.reduce_sum(U_4, axis=1), [-1, 1])\n",
    "M_5 = tf.transpose(tf.sparse_tensor_dense_matmul(UM_t, U_4, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "M_5 = M_5 / tf.reshape(tf.reduce_sum(M_5, axis=1), [-1, 1])\n",
    "\n",
    "# P3: UUM\n",
    "U_2 = tf.transpose(tf.sparse_tensor_dense_matmul(UU_t, u_one_hot, adjoint_b=True)) + 1e-10\n",
    "U_2 = U_2 / tf.reshape(tf.reduce_sum(U_2, axis=1), [-1, 1])\n",
    "M_3 = tf.transpose(tf.sparse_tensor_dense_matmul(UM_t, U_2, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "M_3 = M_3 / tf.reshape(tf.reduce_sum(M_3, axis=1), [-1, 1])\n",
    "\n",
    "# P4: UGrUM\n",
    "G_1 = tf.transpose(tf.sparse_tensor_dense_matmul(UG_t, u_one_hot, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "G_1 = G_1 / tf.reshape(tf.reduce_sum(G_1, axis=1), [-1, 1])\n",
    "U_3 = tf.transpose(tf.sparse_tensor_dense_matmul(UG_t, G_1, adjoint_b=True)) + 1e-10\n",
    "U_3 = U_3 / tf.reshape(tf.reduce_sum(U_3, axis=1), [-1, 1])\n",
    "M_9 = tf.transpose(tf.sparse_tensor_dense_matmul(UM_t, U_3, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "M_9 = M_9 / tf.reshape(tf.reduce_sum(M_9, axis=1), [-1, 1])\n",
    "\n",
    "# P5: ULUM\n",
    "L_1 = tf.transpose(tf.sparse_tensor_dense_matmul(UL_t, u_one_hot, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "L_1 = L_1 / tf.reshape(tf.reduce_sum(L_1, axis=1), [-1, 1])\n",
    "U_4 = tf.transpose(tf.sparse_tensor_dense_matmul(UL_t, L_1, adjoint_b=True)) + 1e-10\n",
    "U_4 = U_4 / tf.reshape(tf.reduce_sum(U_4, axis=1), [-1, 1])\n",
    "M_10 = tf.transpose(tf.sparse_tensor_dense_matmul(UM_t, U_4, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "M_10 = M_10 / tf.reshape(tf.reduce_sum(M_10, axis=1), [-1, 1])\n",
    "\n",
    "# p6: UMDM\n",
    "D_1 = tf.transpose(tf.sparse_tensor_dense_matmul(MD_t, M_1, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "D_1 = D_1 / tf.reshape(tf.reduce_sum(D_1, axis=1), [-1, 1])\n",
    "M_8 = tf.transpose(tf.sparse_tensor_dense_matmul(MD_t, D_1, adjoint_b=True)) + 1e-10\n",
    "M_8 = M_8 / tf.reshape(tf.reduce_sum(M_8, axis=1), [-1, 1])\n",
    "\n",
    "# P7: UMAM\n",
    "A_1 = tf.transpose(tf.sparse_tensor_dense_matmul(MA_t, M_1, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "A_1 = A_1 / tf.reshape(tf.reduce_sum(A_1, axis=1), [-1, 1])\n",
    "M_7 = tf.transpose(tf.sparse_tensor_dense_matmul(MA_t, A_1, adjoint_b=True)) + 1e-10\n",
    "M_7 = M_7 / tf.reshape(tf.reduce_sum(M_7, axis=1), [-1, 1])\n",
    "\n",
    "# P8: UMGeM\n",
    "T_1 = tf.transpose(tf.sparse_tensor_dense_matmul(MT_t, M_1, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "T_1 = T_1 / tf.reshape(tf.reduce_sum(T_1, axis=1), [-1, 1])\n",
    "M_6 = tf.transpose(tf.sparse_tensor_dense_matmul(MT_t, T_1, adjoint_b=True)) + 1e-10\n",
    "M_6 = M_6 / tf.reshape(tf.reduce_sum(M_6, axis=1), [-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1\n",
    "r = tf.nn.softmax(tf.Variable(np.ones(shape=[8]), dtype=tf.float32, trainable=True))\n",
    "y_path = tf.einsum('i,ijk->jk', r, tf.stack([M_2, M_5, M_3, M_9, M_10, M_8, M_7, M_6], axis=0))\n",
    "# L2\n",
    "loss_path = tf.reduce_mean(tf.reduce_sum(- uy * tf.log(y_path), -1))\n",
    "\n",
    "# L3\n",
    "kl_div = tf.reduce_mean(tf.reduce_sum(- tf.log(tf.nn.softmax(y_emb_logit, -1)) * y_path + y_path * tf.log(y_path), -1))\n",
    "\n",
    "reg = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "loss_beta = tf.placeholder(tf.float32, shape=(), name='loss_beta')\n",
    "opt = tf.train.AdamOptimizer().minimize(loss_beta * loss + kl_div + 0.2 * loss_path + 1e-6 * tf.reduce_sum(reg))\n",
    "\n",
    "opt_base = tf.train.AdamOptimizer().minimize(loss + 1e-6 * tf.reduce_sum(reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(u_batch, mask_batch, set_batch, y_val, cutoff=[20, 20, 100]):\n",
    "    hit = []\n",
    "    recall = []\n",
    "    ndcg = []\n",
    "    y_val_argsort = np.argsort(-y_val, axis=-1)[:, :cutoff[2]]\n",
    "    for i in range(len(u_batch)):\n",
    "        has_hit = 0\n",
    "        recall_ = 0.\n",
    "        dcg_max = 0.\n",
    "        dcg = 0.\n",
    "        top_k = y_val_argsort[i]\n",
    "        h = set_batch[i]\n",
    "        for ind, b_rec in enumerate(top_k):\n",
    "            if ind < len(h):\n",
    "                dcg_max += 1. / np.log2(ind + 2)\n",
    "            if b_rec in h:\n",
    "                if ind < cutoff[0]:\n",
    "                    has_hit = 1\n",
    "                if ind < cutoff[1]:\n",
    "                    recall_ += 1.\n",
    "                dcg += 1. / np.log2(ind + 2)\n",
    "        \n",
    "        hit.append(has_hit)\n",
    "        ndcg.append(dcg / dcg_max)\n",
    "        recall_ /= min(len(h), cutoff[1])\n",
    "        recall.append(recall_)\n",
    "    return hit, recall, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 9.00611017846, 1.35025826983\n",
      "0.200338123415, 0.116954880505, 0.143510762807\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "1, 7.67179508037, 0.749819595535\n",
      "0.348126232742, 0.196032418228, 0.208448938664\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2, 7.3971736023, 0.586482166707\n",
      "0.35235277543, 0.194461192373, 0.20839312237\n",
      "3, 7.28640626357, 0.479596162702\n",
      "0.351507466892, 0.194288567614, 0.208869640789\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "4, 7.20969371108, 0.399106677051\n",
      "0.352916314455, 0.194526269849, 0.209488910035\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "5, 7.14933534571, 0.336986821514\n",
      "0.354184277261, 0.195177383708, 0.209841314\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "6, 7.10010188764, 0.287428485112\n",
      "0.352071005917, 0.195152149407, 0.210036507709\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "7, 7.05961912602, 0.246805631228\n",
      "0.35122569738, 0.195603987576, 0.21033382116\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "8, 7.02647221625, 0.213470232514\n",
      "0.351930121161, 0.195908834452, 0.210653735283\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "9, 6.99950100065, 0.186480953887\n",
      "0.35122569738, 0.196221906489, 0.211038107777\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "10, 6.97757232941, 0.164697719453\n",
      "0.351366582136, 0.196634226628, 0.211320016463\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "11, 6.95959760047, 0.147069999025\n",
      "0.351648351648, 0.196994758451, 0.211862844681\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "12, 6.94448534648, 0.132565315712\n",
      "0.352634544942, 0.197133351564, 0.212047784488\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "13, 6.93116673478, 0.120724288461\n",
      "0.353198083967, 0.197517808885, 0.212386431113\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "14, 6.91926482776, 0.11092883476\n",
      "0.356156663849, 0.197748511292, 0.212906841311\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "15, 6.90742548951, 0.10286498788\n",
      "0.354888701043, 0.198318582208, 0.213528382749\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "16, 6.89585373853, 0.0962633056952\n",
      "0.356438433362, 0.19929014446, 0.214244957521\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "17, 6.88370491148, 0.090944933596\n",
      "0.357424626655, 0.20020513522, 0.214835004972\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "18, 6.87132855579, 0.0867673396259\n",
      "0.357283741899, 0.201174272664, 0.215585921211\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "19, 6.85871552562, 0.0835573202721\n",
      "0.359256128487, 0.202307921968, 0.216310967338\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "20, 6.84595554799, 0.0811931570774\n",
      "0.360242321781, 0.203254748299, 0.216947477552\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "21, 6.83359021969, 0.0794749253908\n",
      "0.362073823612, 0.204007890678, 0.217817086471\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "22, 6.82116029928, 0.07840729874\n",
      "0.362355593125, 0.204866332072, 0.218369920412\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "23, 6.80791161082, 0.0779733330951\n",
      "0.363060016906, 0.205659514406, 0.218979736372\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "24, 6.79103889122, 0.0783395973949\n",
      "0.363764440688, 0.20709775727, 0.220009804351\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "25, 6.77209786681, 0.0790547160012\n",
      "0.366159481544, 0.207494895279, 0.221147625711\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "26, 6.75052644111, 0.0801269088511\n",
      "0.367145674838, 0.208947046379, 0.222507134562\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "27, 6.72622032853, 0.0815056996705\n",
      "0.367427444351, 0.210254865023, 0.224046210752\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "28, 6.69901585364, 0.0831490289923\n",
      "0.368836291913, 0.212368943711, 0.225556465032\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "29, 6.66817220052, 0.0851697040437\n",
      "0.370245139476, 0.214283965232, 0.226869187061\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "30, 6.63386847092, 0.0874937773422\n",
      "0.37348548887, 0.215485651125, 0.228824084923\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "31, 6.59580308038, 0.0902521865325\n",
      "0.37461256692, 0.216868055197, 0.230265441718\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "32, 6.5535367244, 0.0939552943717\n",
      "0.377993801071, 0.219008967975, 0.231999363681\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "33, 6.50841108528, 0.0988283604383\n",
      "0.386446886447, 0.222619758017, 0.236293449469\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "34, 6.46209754171, 0.107003212378\n",
      "0.387573964497, 0.223639629715, 0.236974793452\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "35, 6.41261139432, 0.117003230093\n",
      "0.380529726684, 0.222907353562, 0.235203484207\n",
      "36, 6.36712864283, 0.124364163961\n",
      "0.394618202311, 0.229984935467, 0.242166301024\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "37, 6.30036921115, 0.124656474134\n",
      "0.398281205974, 0.231110717211, 0.243863402675\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "38, 6.23169829394, 0.124909109763\n",
      "0.395181741336, 0.229712017914, 0.241882506635\n",
      "39, 6.16786903519, 0.126602818971\n",
      "0.39123696816, 0.229694947482, 0.242130563723\n",
      "40, 6.10094265895, 0.132419302724\n",
      "0.401239785855, 0.234297204093, 0.246347605938\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "41, 6.02808122377, 0.138523189871\n",
      "0.409692871231, 0.237429676416, 0.249268203599\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "42, 5.95313741925, 0.143807442406\n",
      "0.404761904762, 0.233477892783, 0.246479077451\n",
      "43, 5.87865048915, 0.148425593234\n",
      "0.403071287687, 0.234075717671, 0.247219039317\n",
      "44, 5.80060843734, 0.152805713786\n",
      "0.417582417582, 0.239465389009, 0.251432641231\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "45, 5.71989249754, 0.159557893738\n",
      "0.416877993801, 0.238378352746, 0.251565717582\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "46, 5.63971554696, 0.164167354773\n",
      "0.409974640744, 0.236877670864, 0.248724597868\n",
      "47, 5.56410194088, 0.170817037423\n",
      "0.414060298676, 0.238872474297, 0.251523092738\n",
      "48, 5.48350423091, 0.176928844269\n",
      "0.421245421245, 0.240854810761, 0.254102541479\n",
      "INFO:tensorflow:./saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "49, 5.40487579397, 0.185935909684\n",
      "0.42180896027, 0.240834742282, 0.253982192419\n",
      "50, 5.32939633378, 0.19240492542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.416596224289, 0.239240330174, 0.251930299757\n",
      "51, 5.25753857638, 0.199577530494\n",
      "0.41842772612, 0.24043008291, 0.252394136558\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model_proposed.bin\n",
      "0.437447168216, 0.237404500674, 0.258798735565\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_loss = 0.\n",
    "train_auc = 0.\n",
    "train_hit = 0.\n",
    "loss_cnt = 0.\n",
    "best_ndcg = 0.\n",
    "counter = 0\n",
    "base = False  # whether to train the base model alone \n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "\n",
    "# sess.graph.finalize()\n",
    "for epoch in range(100):\n",
    "    train_loss = 0.\n",
    "    train_kl = 0.\n",
    "    loss_cnt = 0.\n",
    "\n",
    "    for i in range(0, user_biz_train.shape[0], batch_size):\n",
    "        uy_batch = user_biz_train[i:i+batch_size, :]\n",
    "        ux_batch = user_non_zero_ind[i:i+batch_size]\n",
    "        if base:\n",
    "            _, loss_val, kl_val = sess.run([opt_base, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch})\n",
    "        else:\n",
    "            _, loss_val, kl_val = sess.run([opt, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch, loss_beta: max(2 * (0.9 ** epoch), 0.2)})\n",
    "\n",
    "        train_loss += loss_val\n",
    "        train_kl += kl_val\n",
    "        loss_cnt += 1\n",
    "\n",
    "    print(\"{}, {}, {}\".format(epoch, train_loss/loss_cnt, train_kl/loss_cnt))\n",
    "\n",
    "    val_hit = []\n",
    "    val_recall = []\n",
    "    val_ndcg = []\n",
    "    for i in range(0, len(val_set_u), batch_size):\n",
    "        u_batch = val_set_u[i:i+batch_size]\n",
    "        mask_batch = val_set_mask[i:i+batch_size]\n",
    "        set_batch = val_set_set[i:i+batch_size]\n",
    "        y_val = sess.run(y_inference, feed_dict={ux: u_batch})\n",
    "        y_val *= np.array(mask_batch)\n",
    "        hit_, recall_, ndcg_ = evaluate(u_batch, mask_batch, set_batch, y_val, cutoff=[5, 20, 100])\n",
    "        val_hit += hit_\n",
    "        val_recall += recall_\n",
    "        val_ndcg += ndcg_\n",
    "    val_hit = np.array(val_hit)\n",
    "    val_recall = np.array(val_recall)\n",
    "    val_ndcg = np.array(val_ndcg)\n",
    "    print(\"{}, {}, {}\".format(val_hit.mean(), val_recall.mean(), val_ndcg.mean()))\n",
    "    \n",
    "    if val_ndcg.mean() >= best_ndcg:\n",
    "        best_ndcg = val_ndcg.mean()\n",
    "        counter = 0\n",
    "        saver.save(sess, './saved_model_%s.bin'%('base' if base else 'proposed'))\n",
    "\n",
    "    counter += 1\n",
    "    if counter > 3:\n",
    "        break\n",
    "\n",
    "saver.restore(sess, './saved_model_%s.bin'%('base' if base else 'proposed'))\n",
    "test_hit = []\n",
    "test_recall = []\n",
    "test_ndcg = []\n",
    "for i in range(0, len(test_set_u), batch_size):\n",
    "    u_batch = test_set_u[i:i+batch_size]\n",
    "    mask_batch = test_set_mask[i:i+batch_size]\n",
    "    set_batch = test_set_set[i:i+batch_size]\n",
    "    y_val = sess.run(y_inference, feed_dict={ux: u_batch})\n",
    "    y_val *= np.array(mask_batch)\n",
    "    hit_, recall_, ndcg_ = evaluate(u_batch, mask_batch, set_batch, y_val, cutoff=[5, 20, 100])\n",
    "    test_hit += hit_\n",
    "    test_recall += recall_\n",
    "    test_ndcg += ndcg_\n",
    "test_hit = np.array(test_hit)\n",
    "test_recall = np.array(test_recall)\n",
    "test_ndcg = np.array(test_ndcg)\n",
    "print(\"{}, {}, {}\".format(test_hit.mean(), test_recall.mean(), test_ndcg.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 9.06072061126, 1.44003292247\n",
      "0.20583262891, 0.116154571115, 0.147707240746\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "1, 7.67372899013, 0.978538833223\n",
      "0.348267117498, 0.195468570116, 0.207826062979\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2, 7.34630424053, 1.04444442622\n",
      "0.353761622992, 0.195245293881, 0.207425906501\n",
      "3, 7.22505988301, 1.09021588005\n",
      "0.35235277543, 0.19451892674, 0.208453443263\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "4, 7.14264832316, 1.11231974761\n",
      "0.352634544942, 0.194537610614, 0.208701231752\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "5, 7.07674529531, 1.12720642487\n",
      "0.352211890673, 0.194662059005, 0.209462720983\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "6, 7.02142192222, 1.14556811146\n",
      "0.352071005917, 0.194915371061, 0.209901434631\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "7, 6.97325377851, 1.16941081028\n",
      "0.351789236405, 0.195672529451, 0.210809946921\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "8, 6.92911203917, 1.19665753358\n",
      "0.351648351648, 0.196373797056, 0.211629433404\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "9, 6.88602648984, 1.22707739983\n",
      "0.354184277261, 0.197860509875, 0.213143226981\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "10, 6.8411063074, 1.26127066107\n",
      "0.356156663849, 0.199183948862, 0.214875111339\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "11, 6.79137792244, 1.30037506958\n",
      "0.352071005917, 0.201764282292, 0.21687645706\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "12, 6.73374535157, 1.34409431885\n",
      "0.354325162017, 0.205035500648, 0.219503480895\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "13, 6.66558348596, 1.3942928282\n",
      "0.359537897999, 0.208022707888, 0.22207838352\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "14, 6.58486444456, 1.4517774539\n",
      "0.357565511412, 0.211565689427, 0.224322545934\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "15, 6.49096399599, 1.52114026504\n",
      "0.357142857143, 0.213098977542, 0.226872926976\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "16, 6.3830580024, 1.60008197969\n",
      "0.364327979713, 0.213639941215, 0.229410107432\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "17, 6.25810018316, 1.68925586262\n",
      "0.37123133277, 0.216486801668, 0.230824263104\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "18, 6.11387823294, 1.78808949123\n",
      "0.373203719358, 0.218524665669, 0.232529495233\n",
      "INFO:tensorflow:./saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "19, 5.94868595536, 1.89844683913\n",
      "0.371794871795, 0.220979917883, 0.232240249453\n",
      "20, 5.76440240241, 2.0213922477\n",
      "0.37123133277, 0.217706194518, 0.231503306245\n",
      "21, 5.56538108448, 2.1577426614\n",
      "0.365032403494, 0.217122039181, 0.229623018105\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model_base.bin\n",
      "0.390955198648, 0.217447900803, 0.238138111101\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_loss = 0.\n",
    "train_auc = 0.\n",
    "train_hit = 0.\n",
    "loss_cnt = 0.\n",
    "best_ndcg = 0.\n",
    "counter = 0\n",
    "base = True  # whether to train the base model alone \n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "\n",
    "# sess.graph.finalize()\n",
    "for epoch in range(100):\n",
    "    train_loss = 0.\n",
    "    train_kl = 0.\n",
    "    loss_cnt = 0.\n",
    "\n",
    "    for i in range(0, user_biz_train.shape[0], batch_size):\n",
    "        uy_batch = user_biz_train[i:i+batch_size, :]\n",
    "        ux_batch = user_non_zero_ind[i:i+batch_size]\n",
    "        if base:\n",
    "            _, loss_val, kl_val = sess.run([opt_base, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch})\n",
    "        else:\n",
    "            _, loss_val, kl_val = sess.run([opt, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch, loss_beta: max(0.9 ** epoch, 0.1)})\n",
    "\n",
    "        train_loss += loss_val\n",
    "        train_kl += kl_val\n",
    "        loss_cnt += 1\n",
    "\n",
    "    print(\"{}, {}, {}\".format(epoch, train_loss/loss_cnt, train_kl/loss_cnt))\n",
    "\n",
    "    val_hit = []\n",
    "    val_recall = []\n",
    "    val_ndcg = []\n",
    "    for i in range(0, len(val_set_u), batch_size):\n",
    "        u_batch = val_set_u[i:i+batch_size]\n",
    "        mask_batch = val_set_mask[i:i+batch_size]\n",
    "        set_batch = val_set_set[i:i+batch_size]\n",
    "        y_val = sess.run(y_inference, feed_dict={ux: u_batch})\n",
    "        y_val *= np.array(mask_batch)\n",
    "        hit_, recall_, ndcg_ = evaluate(u_batch, mask_batch, set_batch, y_val, cutoff=[5, 20, 100])\n",
    "        val_hit += hit_\n",
    "        val_recall += recall_\n",
    "        val_ndcg += ndcg_\n",
    "    val_hit = np.array(val_hit)\n",
    "    val_recall = np.array(val_recall)\n",
    "    val_ndcg = np.array(val_ndcg)\n",
    "    print(\"{}, {}, {}\".format(val_hit.mean(), val_recall.mean(), val_ndcg.mean()))\n",
    "    \n",
    "    if val_ndcg.mean() >= best_ndcg:\n",
    "        best_ndcg = val_ndcg.mean()\n",
    "        counter = 0\n",
    "        saver.save(sess, './saved_model_%s.bin'%('base' if base else 'proposed'))\n",
    "\n",
    "    counter += 1\n",
    "    if counter > 3:\n",
    "        break\n",
    "\n",
    "saver.restore(sess, './saved_model_%s.bin'%('base' if base else 'proposed'))\n",
    "test_hit = []\n",
    "test_recall = []\n",
    "test_ndcg = []\n",
    "for i in range(0, len(test_set_u), batch_size):\n",
    "    u_batch = test_set_u[i:i+batch_size]\n",
    "    mask_batch = test_set_mask[i:i+batch_size]\n",
    "    set_batch = test_set_set[i:i+batch_size]\n",
    "    y_val = sess.run(y_inference, feed_dict={ux: u_batch})\n",
    "    y_val *= np.array(mask_batch)\n",
    "    hit_, recall_, ndcg_ = evaluate(u_batch, mask_batch, set_batch, y_val, cutoff=[5, 20, 100])\n",
    "    test_hit += hit_\n",
    "    test_recall += recall_\n",
    "    test_ndcg += ndcg_\n",
    "test_hit = np.array(test_hit)\n",
    "test_recall = np.array(test_recall)\n",
    "test_ndcg = np.array(test_ndcg)\n",
    "print(\"{}, {}, {}\".format(test_hit.mean(), test_recall.mean(), test_ndcg.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf12",
   "language": "python",
   "name": "tf12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
