{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import shutil\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from collections import Counter\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.6  # train:val:test = ratio:(1-ratio)/2:(1-ratio)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('yelp.mat')\n",
    "UB, UU, UCom, BCat, BCity = (x.tocoo() for x in list(mat['relation'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_biz = defaultdict(list)\n",
    "for u, b in zip(UB.row, UB.col):\n",
    "    user_biz[u].append(b)\n",
    "\n",
    "user_biz_train = defaultdict(list)\n",
    "user_biz_val = defaultdict(list)\n",
    "user_biz_test = defaultdict(list)\n",
    "train_set = set()\n",
    "val_set = set()\n",
    "test_set = set()\n",
    "for u in user_biz:\n",
    "    if len(user_biz[u]) >= 5:\n",
    "        val = int(len(user_biz[u]) * ratio)\n",
    "        test = int(len(user_biz[u]) * (ratio+(1-ratio)/2))\n",
    "        random.shuffle(user_biz[u])\n",
    "        user_biz_train[u] = user_biz[u][:val]\n",
    "        for b in user_biz[u][:val]:\n",
    "            train_set.add((u, b))\n",
    "        user_biz_val[u] = user_biz[u][val:test]\n",
    "        for b in user_biz[u][val:test]:\n",
    "            val_set.add((u, b))\n",
    "        user_biz_test[u] = user_biz[u][test:]\n",
    "        for b in user_biz[u][test:]:\n",
    "            test_set.add((u, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = []\n",
    "val_mask = []\n",
    "test_mask = []\n",
    "for ind, ub in enumerate(zip(UB.row, UB.col)):\n",
    "    if ub in train_set:\n",
    "        train_mask.append(ind)\n",
    "    if ub in val_set:\n",
    "        val_mask.append(ind)\n",
    "    if ub in test_set:\n",
    "        test_mask.append(ind)\n",
    "train_mask = np.array(train_mask)\n",
    "val_mask = np.array(val_mask)\n",
    "test_mask = np.array(test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_biz_train = np.zeros([UB.shape[0], UB.shape[1]])\n",
    "for u, b in zip(UB.row[train_mask], UB.col[train_mask]):\n",
    "    user_biz_train[u, b] += 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out cold start users and items\n",
    "sum_tmp = np.sum(user_biz_train, -1)\n",
    "user_non_zero_ind = np.array([i for i in range(UB.shape[0]) if sum_tmp[i] > 0])\n",
    "user_non_zero_set = set(user_non_zero_ind)\n",
    "\n",
    "sum_tmp = np.sum(user_biz_train[user_non_zero_ind, :], 0)\n",
    "biz_non_zero_mask = (sum_tmp != 0).astype(float)\n",
    "biz_non_zero_set = set([i for i in range(UB.shape[1]) if sum_tmp[i] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_biz_val = np.zeros([UB.shape[0], UB.shape[1]])\n",
    "user_biz_val_dict = defaultdict(list)\n",
    "for u, b in zip(UB.row[val_mask], UB.col[val_mask]):\n",
    "    if u in user_non_zero_set and b in biz_non_zero_set:\n",
    "        user_biz_val[u, b] += 1.\n",
    "    user_biz_val_dict[u].append(b)\n",
    "\n",
    "user_biz_test = np.zeros([UB.shape[0], UB.shape[1]])\n",
    "user_biz_test_dict = defaultdict(list)\n",
    "for u, b in zip(UB.row[test_mask], UB.col[test_mask]):\n",
    "    if u in user_non_zero_set and b in biz_non_zero_set:\n",
    "        user_biz_test[u, b] += 1.\n",
    "    user_biz_test_dict[u].append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data structure needed for the convenience of evaluation\n",
    "val_set_u = list(user_non_zero_set)\n",
    "val_set_mask = []\n",
    "val_set_set = []\n",
    "for user in val_set_u:\n",
    "    val_set_mask.append(biz_non_zero_mask - user_biz_train[user, :] - user_biz_test[user, :])\n",
    "    val_set_set.append(set(user_biz_val_dict[user]))\n",
    "\n",
    "test_set_u = list(user_non_zero_set)\n",
    "test_set_mask = []\n",
    "test_set_set = []\n",
    "for user in test_set_u:\n",
    "    test_set_mask.append(biz_non_zero_mask - user_biz_train[user, :] - user_biz_val[user, :])\n",
    "    test_set_set.append(set(user_biz_test_dict[user]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the training data\n",
    "user_biz_train = user_biz_train[user_non_zero_ind, :]\n",
    "user_biz_train = user_biz_train/np.sum(user_biz_train, -1).reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINs & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse tensors for adjacency matrices\n",
    "UB_train = tf.SparseTensor(indices=np.array([UB.row[train_mask], UB.col[train_mask]]).transpose(),\n",
    "                           values=np.ones(len(train_mask)).astype(np.float32),\n",
    "                           dense_shape=UB.shape)\n",
    "UU_t = tf.SparseTensor(indices=np.array([UU.row, UU.col]).transpose(), values=UU.data.astype(np.float32), dense_shape=UU.shape)\n",
    "UCom_t = tf.SparseTensor(indices=np.array([UCom.row, UCom.col]).transpose(), values=UCom.data.astype(np.float32), dense_shape=UCom.shape)\n",
    "BCat_t = tf.SparseTensor(indices=np.array([BCat.row, BCat.col]).transpose(), values=BCat.data.astype(np.float32), dense_shape=BCat.shape)\n",
    "BCity_t = tf.SparseTensor(indices=np.array([BCity.row, BCity.col]).transpose(),\n",
    "                          values=BCity.data.astype(np.float32),\n",
    "                          dense_shape=BCity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paras\n",
    "num_U = UB.shape[0]\n",
    "num_B = UB.shape[1]\n",
    "num_Com = UCom.shape[1]\n",
    "num_Cat = BCat.shape[1]\n",
    "num_City = BCity.shape[1]\n",
    "\n",
    "dim = 64\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "U_embeddings = tf.get_variable(\"U_embeddings\", [num_U, dim], trainable=True, regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "U_b = tf.get_variable(\"U_b\", [num_U, dim])\n",
    "B_embeddings = tf.get_variable(\"B_embeddings\", [num_B, dim], trainable=True, regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "B_b = tf.get_variable(\"B_b\", [num_U, dim])\n",
    "Com_embeddings = tf.get_variable(\"Com_embeddings\", [num_Com, dim], regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "Cat_embeddings = tf.get_variable(\"Cat_embeddings\", [num_Cat, dim], regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "City_embeddings = tf.get_variable(\"City_embeddings\", [num_City, dim], regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "U_embeddings_mlp = tf.get_variable(\"U_embeddings_mlp\", [num_U, dim], trainable=True, regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "B_embeddings_mlp = tf.get_variable(\"B_embeddings_mlp\", [num_U, dim], trainable=True, regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user & item lookup table\n",
    "U_vec = U_embeddings + tf.sparse_tensor_dense_matmul(tf.sparse_softmax(UB_train), B_embeddings) + \\\n",
    "        tf.sparse_tensor_dense_matmul(tf.sparse_softmax(UU_t), U_embeddings)\n",
    "U_vec = tf.nn.tanh(U_vec)\n",
    "\n",
    "B_vec = B_embeddings + tf.sparse_tensor_dense_matmul(tf.sparse_softmax(tf.sparse_transpose(UB_train)), U_embeddings) + \\\n",
    "        tf.sparse_tensor_dense_matmul(tf.sparse_softmax(BCity_t), City_embeddings) + \\\n",
    "        tf.sparse_tensor_dense_matmul(tf.sparse_softmax(BCat_t), Cat_embeddings)\n",
    "B_vec = tf.nn.tanh(B_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders\n",
    "ux = tf.placeholder(tf.int32, shape=(None,))\n",
    "uy = tf.placeholder(tf.float32, shape=(None, UB.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvec = tf.tile(tf.expand_dims(tf.nn.embedding_lookup(U_vec, ux), 1), [1, UB.shape[1], 1])\n",
    "bvec = tf.tile(tf.expand_dims(tf.nn.embedding_lookup(B_vec, tf.range(0, UB.shape[1])), 0), [tf.shape(uvec)[0], 1, 1])\n",
    "\n",
    "x = tf.concat([uvec * bvec], axis=-1)\n",
    "y_emb_logit = tf.squeeze(tf.layers.dense(x, 1, name='output_2', kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1)))\n",
    "y_inference = tf.nn.softmax(y_emb_logit, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learnable adjacency matrices\n",
    "\n",
    "UB_e = tf.Variable(np.zeros(shape=len(train_mask)), trainable=True, dtype=tf.float32)\n",
    "B_p = tf.Variable(np.zeros(shape=UB.shape[1]), trainable=True, dtype=tf.float32)\n",
    "UU_e = tf.Variable(np.zeros(shape=len(UU.row)), trainable=True, dtype=tf.float32)\n",
    "BCat_e = tf.Variable(np.zeros(shape=len(BCat.row)), trainable=True, dtype=tf.float32)\n",
    "BCity_e = tf.Variable(np.zeros(shape=len(BCity.row)), trainable=True, dtype=tf.float32)\n",
    "\n",
    "\n",
    "UB_t = tf.SparseTensor(indices=np.array([UB.row[train_mask], UB.col[train_mask]]).transpose(),\n",
    "                       values=tf.nn.softplus(UB_e),\n",
    "                       dense_shape=UB.shape)\n",
    "UU_t = tf.SparseTensor(indices=np.array([UU.row, UU.col]).transpose(),\n",
    "                       values=tf.nn.softplus(UU_e),\n",
    "                       dense_shape=UU.shape)\n",
    "BCat_t = tf.SparseTensor(indices=np.array([BCat.row, BCat.col]).transpose(),\n",
    "                         values=tf.nn.softplus(BCat_e),\n",
    "                         dense_shape=BCat.shape)\n",
    "BCity_t = tf.SparseTensor(indices=np.array([BCity.row, BCity.col]).transpose(),\n",
    "                          values=tf.nn.softplus(BCity_e),\n",
    "                          dense_shape=BCity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_one_hot = tf.one_hot(ux, depth=UB.shape[0])\n",
    "\n",
    "# meta-paths\n",
    "\n",
    "# P1: UBUB\n",
    "B_1 = tf.transpose(tf.sparse_tensor_dense_matmul(UB_t, u_one_hot, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "B_1 = B_1 / tf.reshape(tf.reduce_sum(B_1, axis=1), [-1, 1])\n",
    "U_1 = tf.transpose(tf.sparse_tensor_dense_matmul(UB_t, B_1, adjoint_b=True)) + 1e-10\n",
    "U_1 = U_1 / tf.reshape(tf.reduce_sum(U_1, axis=1), [-1, 1])\n",
    "B_2 = tf.transpose(tf.sparse_tensor_dense_matmul(UB_t, U_1, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "B_2 = B_2 / tf.reshape(tf.reduce_sum(B_2, axis=1), [-1, 1])\n",
    "\n",
    "# P2: UBUBU\n",
    "U_3 = tf.transpose(tf.sparse_tensor_dense_matmul(UB_t, B_2, adjoint_b=True)) + 1e-10\n",
    "U_3 = U_3 / tf.reshape(tf.reduce_sum(U_3, axis=1), [-1, 1])\n",
    "B_3 = tf.transpose(tf.sparse_tensor_dense_matmul(UB_t, U_3, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "B_3 = B_3 / tf.reshape(tf.reduce_sum(B_3, axis=1), [-1, 1])\n",
    "\n",
    "# P3: UUB\n",
    "U_2 = tf.transpose(tf.sparse_tensor_dense_matmul(UU_t, u_one_hot, adjoint_b=True)) + 1e-10\n",
    "U_2 = U_2 / tf.reshape(tf.reduce_sum(U_2, axis=1), [-1, 1])\n",
    "B_4 = tf.transpose(tf.sparse_tensor_dense_matmul(UB_t, U_2, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "B_4 = B_4 / tf.reshape(tf.reduce_sum(B_4, axis=1), [-1, 1])\n",
    "\n",
    "# P4: UBCatB\n",
    "Cat = tf.transpose(tf.sparse_tensor_dense_matmul(BCat_t, B_1, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "Cat = Cat / tf.reshape(tf.reduce_sum(Cat, axis=1), [-1, 1])\n",
    "B_5 = tf.transpose(tf.sparse_tensor_dense_matmul(BCat_t, Cat, adjoint_b=True)) + 1e-10\n",
    "B_5 = B_5 / tf.reshape(tf.reduce_sum(B_5, axis=1), [-1, 1])\n",
    "\n",
    "# P5: UBCityB\n",
    "City = tf.transpose(tf.sparse_tensor_dense_matmul(BCity_t, B_1, adjoint_a=True, adjoint_b=True)) + 1e-10\n",
    "City = City / tf.reshape(tf.reduce_sum(City, axis=1), [-1, 1])\n",
    "B_6 = tf.transpose(tf.sparse_tensor_dense_matmul(BCity_t, City, adjoint_b=True)) + 1e-10\n",
    "B_6 = B_6 / tf.reshape(tf.reduce_sum(B_6, axis=1), [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tf.nn.softmax(tf.Variable(np.ones(shape=[5]), dtype=tf.float32, trainable=True))\n",
    "y_path = tf.einsum('i,ijk->jk', r, tf.stack([B_2, B_3, B_4, B_5, B_6], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-d16bac2f3230>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# L1\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_emb_logit, labels=uy))\n",
    "# L2\n",
    "loss_path = tf.reduce_mean(tf.reduce_sum(- uy * tf.log(y_path), -1))\n",
    "# L3\n",
    "kl_div = tf.reduce_mean(tf.reduce_sum(- tf.log(tf.nn.softmax(y_emb_logit, -1)) * y_path + y_path * tf.log(y_path), -1))\n",
    "\n",
    "reg = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "opt = tf.train.AdamOptimizer().minimize(loss + kl_div + loss_path + 1e-5 * tf.reduce_sum(reg))\n",
    "opt_p = tf.train.AdamOptimizer().minimize(0.1 * loss + kl_div + 0.1 * loss_path + 1e-5 * tf.reduce_sum(reg)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_base = tf.train.AdamOptimizer().minimize(loss + 1e-5 * tf.reduce_sum(reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(u_batch, mask_batch, set_batch, y_val, cutoff=[20, 20, 100]):\n",
    "    hit = []\n",
    "    recall = []\n",
    "    ndcg = []\n",
    "    y_val_argsort = np.argsort(-y_val, axis=-1)[:, :cutoff[2]]\n",
    "    for i in range(len(u_batch)):\n",
    "        has_hit = 0\n",
    "        recall_ = 0.\n",
    "        dcg_max = 0.\n",
    "        dcg = 0.\n",
    "        top_k = y_val_argsort[i]\n",
    "        h = set_batch[i]\n",
    "        for ind, b_rec in enumerate(top_k):\n",
    "            if ind < len(h):\n",
    "                dcg_max += 1. / np.log2(ind + 2)\n",
    "            if b_rec in h:\n",
    "                if ind < cutoff[0]:\n",
    "                    has_hit = 1\n",
    "                if ind < cutoff[1]:\n",
    "                    recall_ += 1.\n",
    "                dcg += 1. / np.log2(ind + 2)\n",
    "        \n",
    "        hit.append(has_hit)\n",
    "        ndcg.append(dcg / dcg_max)\n",
    "        recall_ /= min(len(h), cutoff[1])\n",
    "        recall.append(recall_)\n",
    "    return hit, recall, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 9.52600564209, 0.819747501729\n",
      "0.0911182696733, 0.0212318609073, 0.0236917557847\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "1, 9.27545319351, 0.661956438831\n",
      "0.152017180549, 0.0378696462174, 0.0445312683817\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2, 8.97072987463, 0.582255990482\n",
      "0.206319987728, 0.056016515972, 0.0619171287606\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "3, 8.68978257273, 0.538929787921\n",
      "0.239760699494, 0.0668493110019, 0.0715801799243\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "4, 8.44811814439, 0.508720795898\n",
      "0.250805338242, 0.0725702561961, 0.0775220789355\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "5, 8.23964913686, 0.48322818326\n",
      "0.254793680012, 0.0740050364649, 0.0811019711738\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "6, 8.05793513504, 0.469345102123\n",
      "0.258015032981, 0.0759699652289, 0.0840202043943\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "7, 7.90116052067, 0.461664257388\n",
      "0.266451909802, 0.0810138186912, 0.0868605532833\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "8, 7.76116055133, 0.463835234443\n",
      "0.270593649333, 0.0820903548238, 0.0887632680706\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "9, 7.63315522904, 0.471581315877\n",
      "0.279030526154, 0.0860599068758, 0.0905130966953\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "10, 7.51226615438, 0.483623164249\n",
      "0.286700414174, 0.0886486719722, 0.0923904335695\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "11, 7.39284659367, 0.498980542608\n",
      "0.293756711152, 0.0907442637561, 0.0947821022794\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "12, 7.27126779743, 0.517022463621\n",
      "0.300199417089, 0.0933516606339, 0.0964994871363\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "13, 7.14233846758, 0.537660646672\n",
      "0.302040190213, 0.0944994233434, 0.0981129114277\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "14, 7.00976920595, 0.556414743557\n",
      "0.30848289615, 0.0973974955055, 0.0999183690469\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "15, 6.86801134839, 0.581769147048\n",
      "0.308176100629, 0.0977375803056, 0.101086452707\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "16, 6.71902978187, 0.605505728254\n",
      "0.310937260316, 0.0976421148776, 0.10177074481\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "17, 6.8086262731, 0.516418166605\n",
      "0.30449455438, 0.0956284823489, 0.101056814217\n",
      "18, 6.95330743229, 0.401972919995\n",
      "0.311857646878, 0.0987919205403, 0.102288400755\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "19, 6.99345016479, 0.346736530755\n",
      "0.313391624482, 0.0987901503843, 0.102215978845\n",
      "20, 7.00632539917, 0.305423098744\n",
      "0.315232397607, 0.0993917406242, 0.103220276799\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "21, 6.99921308779, 0.274232787683\n",
      "0.314312011045, 0.099118875984, 0.103887294247\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "22, 6.98697096226, 0.24598156354\n",
      "0.31645957969, 0.100018583691, 0.104267075165\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "23, 6.96168645223, 0.224646424254\n",
      "0.317840159534, 0.0997049276458, 0.10474648059\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "24, 6.93838749212, 0.204674801698\n",
      "0.319220739377, 0.101424005143, 0.10514767812\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "25, 6.9082764317, 0.189443290526\n",
      "0.32014112594, 0.101151668098, 0.10579761222\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "26, 6.87648829759, 0.176864200071\n",
      "0.319834330419, 0.101053474605, 0.105959402596\n",
      "INFO:tensorflow:./ckpt/saved_model_proposed.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "27, 6.84434472346, 0.166739019255\n",
      "0.318913943856, 0.100972715309, 0.105757614262\n",
      "28, 6.81330057219, 0.158609372436\n",
      "0.317379966252, 0.10077396971, 0.105308086685\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt/saved_model_proposed.bin\n",
      "0.355269213069, 0.102983825328, 0.109304468154\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0.\n",
    "train_auc = 0.\n",
    "train_hit = 0.\n",
    "loss_cnt = 0.\n",
    "best_ndcg = 0.\n",
    "counter = 0\n",
    "base = False  # whether to train the base model alone \n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "\n",
    "# sess.graph.finalize()\n",
    "for epoch in range(50):\n",
    "    train_loss = 0.\n",
    "    train_kl = 0.\n",
    "    loss_cnt = 0.\n",
    "\n",
    "    for i in range(0, user_biz_train.shape[0], batch_size):\n",
    "        uy_batch = user_biz_train[i:i+batch_size, :]\n",
    "        ux_batch = user_non_zero_ind[i:i+batch_size]\n",
    "        if base:\n",
    "            _, loss_val, kl_val = sess.run([opt_base, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch})\n",
    "        else:\n",
    "            if epoch < 17:\n",
    "                _, loss_val, kl_val = sess.run([opt, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch})\n",
    "            else:\n",
    "                _, loss_val, kl_val = sess.run([opt_p, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch})\n",
    "\n",
    "        train_loss += loss_val\n",
    "        train_kl += kl_val\n",
    "        loss_cnt += 1\n",
    "\n",
    "    print(\"{}, {}, {}\".format(epoch, train_loss/loss_cnt, train_kl/loss_cnt))\n",
    "\n",
    "    val_hit = []\n",
    "    val_recall = []\n",
    "    val_ndcg = []\n",
    "    for i in range(0, len(val_set_u), batch_size):\n",
    "        u_batch = val_set_u[i:i+batch_size]\n",
    "        mask_batch = val_set_mask[i:i+batch_size]\n",
    "        set_batch = val_set_set[i:i+batch_size]\n",
    "        y_val = sess.run(y_inference, feed_dict={ux: u_batch})\n",
    "        y_val *= np.array(mask_batch)\n",
    "        hit_, recall_, ndcg_ = evaluate(u_batch, mask_batch, set_batch, y_val)\n",
    "        val_hit += hit_\n",
    "        val_recall += recall_\n",
    "        val_ndcg += ndcg_\n",
    "    val_hit = np.array(val_hit)\n",
    "    val_recall = np.array(val_recall)\n",
    "    val_ndcg = np.array(val_ndcg)\n",
    "    print(\"{}, {}, {}\".format(val_hit.mean(), val_recall.mean(), val_ndcg.mean()))\n",
    "    \n",
    "    if val_ndcg.mean() >= best_ndcg:\n",
    "        best_ndcg = val_ndcg.mean()\n",
    "        counter = 0\n",
    "        saver.save(sess, './ckpt/saved_model_%s.bin'%('base' if base else 'proposed'))\n",
    "\n",
    "    counter += 1\n",
    "    if counter > 2:\n",
    "        break\n",
    "\n",
    "saver.restore(sess, './ckpt/saved_model_%s.bin'%('base' if base else 'proposed'))\n",
    "test_hit = []\n",
    "test_recall = []\n",
    "test_ndcg = []\n",
    "for i in range(0, len(test_set_u), batch_size):\n",
    "    u_batch = test_set_u[i:i+batch_size]\n",
    "    mask_batch = test_set_mask[i:i+batch_size]\n",
    "    set_batch = test_set_set[i:i+batch_size]\n",
    "    y_val = sess.run(y_inference, feed_dict={ux: u_batch})\n",
    "    y_val *= np.array(mask_batch)\n",
    "    hit_, recall_, ndcg_ = evaluate(u_batch, mask_batch, set_batch, y_val)\n",
    "    test_hit += hit_\n",
    "    test_recall += recall_\n",
    "    test_ndcg += ndcg_\n",
    "test_hit = np.array(test_hit)\n",
    "test_recall = np.array(test_recall)\n",
    "test_ndcg = np.array(test_ndcg)\n",
    "print(\"{}, {}, {}\".format(test_hit.mean(), test_recall.mean(), test_ndcg.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 9.53830493665, 0.863836005622\n",
      "0.113054149409, 0.0278419288381, 0.0343883493011\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "1, 9.28183423772, 0.749057843989\n",
      "0.151250191747, 0.0384134763391, 0.0456546621902\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2, 8.90374230404, 0.69897037803\n",
      "0.209541340696, 0.0563067861711, 0.0615108513308\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "3, 8.55251574984, 0.741602438922\n",
      "0.240374290535, 0.0678058194534, 0.0734374446276\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "4, 8.25497357986, 0.833936821012\n",
      "0.253259702408, 0.0723386967016, 0.0796932655933\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "5, 8.01133123099, 0.937372676882\n",
      "0.260469397147, 0.0753592186595, 0.0830772757265\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "6, 7.80006636358, 1.03724013181\n",
      "0.266145114281, 0.0781635917252, 0.0855953232298\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "7, 7.60586099999, 1.12515197548\n",
      "0.275348979905, 0.0817452952096, 0.0877257882102\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "8, 7.42355212043, 1.21847289275\n",
      "0.280104310477, 0.0838211078433, 0.0898107525765\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "9, 7.24631791021, 1.31045832237\n",
      "0.283479061206, 0.0860721771647, 0.0909620265202\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "10, 7.0670234035, 1.41268032205\n",
      "0.287620800736, 0.0868711645082, 0.0926991514687\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "11, 6.88335734723, 1.51641340115\n",
      "0.291302346986, 0.0896429245664, 0.0934799830295\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "12, 6.69276189337, 1.63653518873\n",
      "0.294216904433, 0.0909584633308, 0.093974171849\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "13, 6.49371794626, 1.76329034567\n",
      "0.288080994017, 0.0886769313093, 0.094780534616\n",
      "INFO:tensorflow:./ckpt/saved_model_base.bin is not in all_model_checkpoint_paths. Manually adding it.\n",
      "14, 6.28442054169, 1.90122957674\n",
      "0.287160607455, 0.0875844808009, 0.0939880267018\n",
      "15, 6.06531843017, inf\n",
      "0.290228562663, 0.087278348491, 0.0946627758179\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt/saved_model_base.bin\n",
      "0.313084828961, 0.0879360939205, 0.0993332317112\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0.\n",
    "train_auc = 0.\n",
    "train_hit = 0.\n",
    "loss_cnt = 0.\n",
    "best_ndcg = 0.\n",
    "counter = 0\n",
    "base = True  # whether to train the base model alone \n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "\n",
    "# sess.graph.finalize()\n",
    "for epoch in range(50):\n",
    "    train_loss = 0.\n",
    "    train_kl = 0.\n",
    "    loss_cnt = 0.\n",
    "\n",
    "    for i in range(0, user_biz_train.shape[0], batch_size):\n",
    "        uy_batch = user_biz_train[i:i+batch_size, :]\n",
    "        ux_batch = user_non_zero_ind[i:i+batch_size]\n",
    "        if base:\n",
    "            _, loss_val, kl_val = sess.run([opt_base, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch})\n",
    "        else:\n",
    "            if epoch < 17:\n",
    "                _, loss_val, kl_val = sess.run([opt, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch})\n",
    "            else:\n",
    "                _, loss_val, kl_val = sess.run([opt_p, loss, kl_div], feed_dict={ux: ux_batch, uy: uy_batch})\n",
    "\n",
    "        train_loss += loss_val\n",
    "        train_kl += kl_val\n",
    "        loss_cnt += 1\n",
    "\n",
    "    print(\"{}, {}, {}\".format(epoch, train_loss/loss_cnt, train_kl/loss_cnt))\n",
    "\n",
    "    val_hit = []\n",
    "    val_recall = []\n",
    "    val_ndcg = []\n",
    "    for i in range(0, len(val_set_u), batch_size):\n",
    "        u_batch = val_set_u[i:i+batch_size]\n",
    "        mask_batch = val_set_mask[i:i+batch_size]\n",
    "        set_batch = val_set_set[i:i+batch_size]\n",
    "        y_val = sess.run(y_inference, feed_dict={ux: u_batch})\n",
    "        y_val *= np.array(mask_batch)\n",
    "        hit_, recall_, ndcg_ = evaluate(u_batch, mask_batch, set_batch, y_val)\n",
    "        val_hit += hit_\n",
    "        val_recall += recall_\n",
    "        val_ndcg += ndcg_\n",
    "    val_hit = np.array(val_hit)\n",
    "    val_recall = np.array(val_recall)\n",
    "    val_ndcg = np.array(val_ndcg)\n",
    "    print(\"{}, {}, {}\".format(val_hit.mean(), val_recall.mean(), val_ndcg.mean()))\n",
    "    \n",
    "    if val_ndcg.mean() >= best_ndcg:\n",
    "        best_ndcg = val_ndcg.mean()\n",
    "        counter = 0\n",
    "        saver.save(sess, './ckpt/saved_model_%s.bin'%('base' if base else 'proposed'))\n",
    "\n",
    "    counter += 1\n",
    "    if counter > 2:\n",
    "        break\n",
    "\n",
    "saver.restore(sess, './ckpt/saved_model_%s.bin'%('base' if base else 'proposed'))\n",
    "test_hit = []\n",
    "test_recall = []\n",
    "test_ndcg = []\n",
    "for i in range(0, len(test_set_u), batch_size):\n",
    "    u_batch = test_set_u[i:i+batch_size]\n",
    "    mask_batch = test_set_mask[i:i+batch_size]\n",
    "    set_batch = test_set_set[i:i+batch_size]\n",
    "    y_val = sess.run(y_inference, feed_dict={ux: u_batch})\n",
    "    y_val *= np.array(mask_batch)\n",
    "    hit_, recall_, ndcg_ = evaluate(u_batch, mask_batch, set_batch, y_val)\n",
    "    test_hit += hit_\n",
    "    test_recall += recall_\n",
    "    test_ndcg += ndcg_\n",
    "test_hit = np.array(test_hit)\n",
    "test_recall = np.array(test_recall)\n",
    "test_ndcg = np.array(test_ndcg)\n",
    "print(\"{}, {}, {}\".format(test_hit.mean(), test_recall.mean(), test_ndcg.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the proposed model for significance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt/saved_model_proposed.bin\n",
      "0.355269213069, 0.102983825328, 0.109304468154\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, './ckpt/saved_model_proposed.bin')\n",
    "test_hit_ = []\n",
    "test_recall_ = []\n",
    "test_ndcg_ = []\n",
    "for i in range(0, len(test_set_u), batch_size):\n",
    "    u_batch = test_set_u[i:i+batch_size]\n",
    "    mask_batch = test_set_mask[i:i+batch_size]\n",
    "    set_batch = test_set_set[i:i+batch_size]\n",
    "    y_val = sess.run(y_inference, feed_dict={ux: u_batch})\n",
    "    y_val *= np.array(mask_batch)\n",
    "    hit_, recall_, ndcg_ = evaluate(u_batch, mask_batch, set_batch, y_val)\n",
    "    test_hit_ += hit_\n",
    "    test_recall_ += recall_\n",
    "    test_ndcg_ += ndcg_\n",
    "test_hit_ = np.array(test_hit_)\n",
    "test_recall_ = np.array(test_recall_)\n",
    "test_ndcg_ = np.array(test_ndcg_)\n",
    "print(\"{}, {}, {}\".format(test_hit_.mean(), test_recall_.mean(), test_ndcg_.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_relResult(statistic=-8.013781477612055, pvalue=1.307992963708771e-15)\n",
      "Ttest_relResult(statistic=-7.86187488249895, pvalue=4.3979860011059035e-15)\n",
      "Ttest_relResult(statistic=-9.053271973976338, pvalue=1.802576566104731e-19)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "print(stats.ttest_rel(test_hit, test_hit_))\n",
    "print(stats.ttest_rel(test_recall, test_recall_))\n",
    "print(stats.ttest_rel(test_ndcg, test_ndcg_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf12",
   "language": "python",
   "name": "tf12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
